{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46ecf35-60b3-4d49-b32d-df00e73040a4",
   "metadata": {},
   "source": [
    "# SEO Tools \n",
    "## Scraping Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32879251-18b3-495b-9d98-c670f6e638a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 URLs in the sitemap.\n",
      "\n",
      "Failed to scrape https://www.schmittchevrolet.com/chevrolet-business-choice/: 404\n",
      "\n",
      " Data saved to sitemap_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Define headers to mimic a real browser since I keep getting a 403 error\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com/\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "}\n",
    "\n",
    "#Get sitemap (XML)\n",
    "sitemap_url = \"https://www.schmittchevrolet.com/page-sitemap.xml\"\n",
    "response = requests.get(sitemap_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    root = ET.fromstring(response.text)\n",
    "\n",
    "    #Extract all URLs from sitemap\n",
    "    namespace = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"}\n",
    "    urls = [elem.text for elem in root.findall(\".//ns:loc\", namespace)]\n",
    "\n",
    "    print(f\"Found {len(urls)} URLs in the sitemap.\\n\")\n",
    "\n",
    "    #Scrape each page for metadata\n",
    "    data = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            page_response = requests.get(url, headers=headers, timeout=10)\n",
    "            if page_response.status_code == 200:\n",
    "                soup = BeautifulSoup(page_response.text, \"html.parser\")\n",
    "\n",
    "                #Extract metadata\n",
    "                title = soup.title.text.strip() if soup.title else None\n",
    "                description = (\n",
    "                    soup.find(\"meta\", attrs={\"name\": \"description\"})[\"content\"].strip()\n",
    "                    if soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "                    else None\n",
    "                )\n",
    "                keywords = (\n",
    "                    soup.find(\"meta\", attrs={\"name\": \"keywords\"})[\"content\"].strip()\n",
    "                    if soup.find(\"meta\", attrs={\"name\": \"keywords\"})\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Determine if each metadata element is missing (1 = missing, 0 = present)\n",
    "                missing_title = 1 if title is None else 0\n",
    "                missing_description = 1 if description is None else 0\n",
    "                missing_keywords = 1 if keywords is None else 0\n",
    "\n",
    "                # Replace None values with placeholders for better readability\n",
    "                title = title if title else \"No Title\"\n",
    "                description = description if description else \"No Description\"\n",
    "                keywords = keywords if keywords else \"No Keywords\"\n",
    "\n",
    "                # Append data to list\n",
    "                data.append([url, title, description, keywords, missing_title, missing_description, missing_keywords])\n",
    "\n",
    "            else:\n",
    "                print(f\"Failed to scrape {url}: {page_response.status_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"URL\", \"Title\", \"Description\", \"Keywords\", \"Missing_Title\", \"Missing_Description\", \"Missing_Keywords\"])\n",
    "\n",
    "    df.to_csv(\"weber_sitemap_metadata.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"\\n Data saved to sitemap_metadata.csv\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: Unable to access sitemap.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
